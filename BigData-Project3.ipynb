{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4d95a2",
   "metadata": {},
   "source": [
    "# Big Data Assignment 3: Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4771c89",
   "metadata": {},
   "source": [
    "Rules:\n",
    "1. All words have to be converted to lowercase\n",
    "2. Words are separated by any number of spaces or tabs\n",
    "3. If any word has any special character (other than a-z and A-Z), then that word is filtered out (dropped): with one exception: if a word ends with {\".\", \";\", \",\", \"?\", \":\"} then drop that special character, and then keep/use it as a proper word\n",
    "4. Therefore a valid word can have only {a-z, A-Z}\n",
    "5. You may use the Spark's collect() for debugging purposes, but your submitted solution should ignore debugging statements. For each transformation, you need to provide a single comment line\n",
    "6. You have to be able to explain your transformations\n",
    "7. Must use some Python functions for transformations\n",
    "8. You must keep transformations as simple as possible and use Python functions\n",
    "\n",
    "You may download the input from here:\n",
    "https://github.com/mahmoudparsian/big-data-mapreduce-course/blob/master/data/205-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad6d5d",
   "metadata": {},
   "source": [
    "### Step 1: Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b663d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1bc457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.4.25:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb62042fb20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441cd33",
   "metadata": {},
   "source": [
    "### Step 2: Input Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb907e0",
   "metadata": {},
   "source": [
    "Create path to text file you wish to run the word count on, then create an RDD from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa99f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"205-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165a5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d898d05",
   "metadata": {},
   "source": [
    "### Step 3: Define Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8862b",
   "metadata": {},
   "source": [
    "1. word_split - this function will split the lines of text into individual word elements, which will need to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a81e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_split(record):\n",
    "    tokens = record.split(\" \")\n",
    "    return (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f24ba",
   "metadata": {},
   "source": [
    "2. make_low - this function will convert all our words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "644fb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_low(record):\n",
    "    low_record = record.lower()\n",
    "    return low_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3938083",
   "metadata": {},
   "source": [
    "3. check_words - this function will check to see if the word fits within the assignment's rules. All non-alphabetical words, like numbers, get dropped. Any word that contains a special character will also get dropped, unless it has one of {\".\", \";\", \",\", \"?\", \":\"} on the end. In that case the word will be kept but the character will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "964f43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_words(record):\n",
    "    spec_char = (\".\", \";\", \",\", \"?\", \":\")\n",
    "    if record.isalpha():\n",
    "        return record\n",
    "    elif record.endswith(spec_char):\n",
    "        return record[0:len(record)-1]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72793e",
   "metadata": {},
   "source": [
    "### Step 4: Begin Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecde97",
   "metadata": {},
   "source": [
    "Use _word_split_ to create word elements instead of line elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21350ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rdd = rdd.flatMap(word_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923a0a3",
   "metadata": {},
   "source": [
    "Use _make_low_ to make each element lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cb407f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_word_rdd = word_rdd.map(make_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73faeebd",
   "metadata": {},
   "source": [
    "Use _check_words_ to drop elements that don't fit in the assignment's rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c833342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rdd = low_word_rdd.map(check_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b7053",
   "metadata": {},
   "source": [
    "### Step 5: Filter RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898015cf",
   "metadata": {},
   "source": [
    "Next we will run three filters on the RDD. \n",
    "\n",
    "1. First filter will remove \"None\" values that were left with the pass command from _check_words_\n",
    "2. Second filter will drop any remaining elements that are not alphabetical words. This is necessary as the _clean_words_ function will still keep values like \"28,\" since it had a special character at the end. \n",
    "3. Third filter checks to make sure any word with less than 4 characters will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c301bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rdd = clean_rdd.filter(lambda x: x if x else False).filter(lambda x: x.isalpha()).filter(lambda x: len(x)>=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda239e9",
   "metadata": {},
   "source": [
    "### Step 6: Create (K, V) Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f013c76",
   "metadata": {},
   "source": [
    "Each word will be made a key with a 1 count as the value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a2b4e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rdd = filter_rdd.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4cdf5",
   "metadata": {},
   "source": [
    "### Step 7: Reduce by Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11944e3",
   "metadata": {},
   "source": [
    "This will sum the counts for each key, resulting in a word count for each word appearing in the document that meets our requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a63f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_rdd = count_rdd.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423065f",
   "metadata": {},
   "source": [
    "### Step 8: Select Top 5 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91df4925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('that', 1330), ('with', 916), ('which', 870), ('they', 699), ('have', 673)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_rdd.takeOrdered(5, key = lambda x: -x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
